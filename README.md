# HetServe-LLMs
This is a repository for organizing papers, codes and other resources related to the topic of distributed serving LLMs.

## LLM Serving Survey
- [Full Stack Optimization of Transformer Inference: a Survey](https://arxiv.org/abs/2302.14017) | UC Berkeley
- [Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems](https://arxiv.org/pdf/2312.15234) | Carnegie Mellon University

## LLM Serving Systems
> tensor parallelism (TP), pipeline parallelism (PP), CPU-GPU offloading (offload)

- [Accelerate](https://github.com/huggingface/accelerate) | HuggingFace ![Github stars](https://img.shields.io/github/stars/huggingface/accelerate.svg) ![Github forks](https://img.shields.io/github/forks/huggingface/accelerate.svg)
  - Prioritized optimization target: xxx
  - Optimization: xxx
  - Main features: xxx
  - Parallel computation: TP, PP, offload
  - Heterogeneous: xxx 
- [vLLM](https://github.com/vllm-project/vllm) | UC Berkeley ![Github stars](https://img.shields.io/github/stars/vllm-project/vllm.svg) ![Github forks](https://img.shields.io/github/forks/vllm-project/vllm.svg)
  - Prioritized optimization target: xxx
  - Optimization: xxx
  - Main features: xxx
  - Parallel computation: xxx
  - Heterogeneous: xxx 
- [llama.cpp](https://github.com/ggerganov/llama.cpp) | Georgi Gerganov ![Github stars](https://img.shields.io/github/stars/ggerganov/llama.cpp.svg) ![Github forks](https://img.shields.io/github/forks/ggerganov/llama.cpp.svg)
  - Prioritized optimization target: xxx
  - Optimization: xxx
  - Main features: xxx
  - Parallel computation: xxx
  - Heterogeneous: xxx 
- [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) | NVIDIA ![Github stars](https://img.shields.io/github/stars/NVIDIA/TensorRT-LLM.svg) ![Github forks](https://img.shields.io/github/forks/NVIDIA/TensorRT-LLM.svg)
  - Prioritized optimization target: xxx
  - Optimization: xxx
  - Main features: xxx
  - Parallel computation: xxx
  - Heterogeneous: xxx 
- [DeepSpeed](https://github.com/microsoft/DeepSpeed) | Microsoft ![Github stars](https://img.shields.io/github/stars/microsoft/DeepSpeed.svg) ![Github forks](https://img.shields.io/github/forks/microsoft/DeepSpeed.svg)
  - Prioritized optimization target: xxx
  - Optimization: xxx
  - Main features: xxx
  - Parallel computation: xxx
  - Heterogeneous: xxx 
- [LightLLM](https://github.com/ModelTC/lightllm) | ModelTC ![Github stars](https://img.shields.io/github/stars/ModelTC/lightllmsvg) ![Github forks](https://img.shields.io/github/forks/ModelTC/lightllm.svg)
  - Prioritized optimization target: xxx
  - Optimization: xxx
  - Main features: xxx
  - Parallel computation: xxx
  - Heterogeneous: xxx 
- [MLC-LLM](https://github.com/mlc-ai/mlc-llm) | CMU ![Github stars](https://img.shields.io/github/stars/huggingface/accelerate.svg) ![Github forks](https://img.shields.io/github/forks/huggingface/accelerate.svg)
  - Prioritized optimization target: xxx
  - Optimization: xxx
  - Main features: xxx
  - Parallel computation: xxx
  - Heterogeneous: xxx 

## Distributed Inference
- [PipeEdge: Pipeline Parallelism for Large-Scale Model Inference on Heterogeneous Edge Devices](https://github.com/usc-isi/PipeEdge) | Purdue University
- [Fast Distributed Inference Serving for Large Language Models](https://arxiv.org/abs/2305.05920) | Peking University
- [Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache](https://arxiv.org/abs/2401.02669) | Alibaba Group
